{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCJc50U11HOM",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b4354f-a346-4b7f-8201-c552afcda455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jax[cuda12] in /usr/local/lib/python3.11/dist-packages (0.4.33)\n",
            "Collecting jax[cuda12]\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax[cuda12])\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12]) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12]) (1.26.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax[cuda12]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax[cuda12]) (1.13.1)\n",
            "Collecting jax-cuda12-plugin<=0.5.0,>=0.5.0 (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_plugin-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax-cuda12-pjrt==0.5.0 (from jax-cuda12-plugin<=0.5.0,>=0.5.0->jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading jax_cuda12_pjrt-0.5.0-py3-none-manylinux2014_x86_64.whl.metadata (348 bytes)\n",
            "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (12.5.82)\n",
            "Collecting nvidia-cuda-nvcc-cu12>=12.6.85 (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12])\n",
            "  Downloading nvidia_cuda_nvcc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (12.5.82)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12<10.0,>=9.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (9.3.0.75)\n",
            "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (11.2.3.61)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (12.5.1.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /usr/local/lib/python3.11/dist-packages (from jax-cuda12-plugin[with_cuda]<=0.5.0,>=0.5.0; extra == \"cuda12\"->jax[cuda12]) (12.5.82)\n",
            "Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl (102.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.5.0-cp311-cp311-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.5.0-py3-none-manylinux2014_x86_64.whl (103.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.5.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.8.61-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, nvidia-cuda-nvcc-cu12, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: nvidia-cuda-nvcc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvcc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvcc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvcc-cu12-12.5.82\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "Successfully installed jax-0.5.0 jax-cuda12-pjrt-0.5.0 jax-cuda12-plugin-0.5.0 jaxlib-0.5.0 nvidia-cuda-nvcc-cu12-12.8.61\n",
            "Collecting diffrax\n",
            "  Downloading diffrax-0.6.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Collecting distrax\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Collecting equinox>=0.11.10 (from diffrax)\n",
            "  Downloading equinox-0.11.11-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: jax>=0.4.38 in /usr/local/lib/python3.11/dist-packages (from diffrax) (0.5.0)\n",
            "Collecting jaxtyping>=0.2.24 (from diffrax)\n",
            "  Downloading jaxtyping-0.2.37-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting lineax>=0.0.5 (from diffrax)\n",
            "  Downloading lineax-0.0.7-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting optimistix>=0.0.7 (from diffrax)\n",
            "  Downloading optimistix-0.0.10-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting typeguard==2.13.3 (from diffrax)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from diffrax) (4.12.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.71)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax) (1.26.4)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.1.88)\n",
            "Requirement already satisfied: jaxlib>=0.1.67 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.5.0)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from distrax) (0.25.0)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax) (1.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.8->distrax) (0.12.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.38->diffrax) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.38->diffrax) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.38->diffrax) (1.13.1)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.24->diffrax)\n",
            "  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (1.17.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (3.1.1)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.6.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability>=0.15.0->distrax) (0.1.9)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.25.6)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.11.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability>=0.15.0->distrax) (25.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow-probability>=0.15.0->distrax) (1.17.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (3.21.0)\n",
            "Downloading diffrax-0.6.2-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Downloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading equinox-0.11.11-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.37-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lineax-0.0.7-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optimistix-0.0.10-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: wadler-lindig, typeguard, jaxtyping, equinox, lineax, distrax, optimistix, diffrax\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.4.1\n",
            "    Uninstalling typeguard-4.4.1:\n",
            "      Successfully uninstalled typeguard-4.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inflect 7.5.0 requires typeguard>=4.0.1, but you have typeguard 2.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diffrax-0.6.2 distrax-0.1.5 equinox-0.11.11 jaxtyping-0.2.37 lineax-0.0.7 optimistix-0.0.10 typeguard-2.13.3 wadler-lindig-0.1.3\n"
          ]
        }
      ],
      "source": [
        "! pip install -U \"jax[cuda12]\"\n",
        "# ! pip install -U jax\n",
        "! pip install -U  diffrax flax distrax optax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import distrax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as jrnd\n",
        "import optax\n",
        "from flax import nnx\n",
        "\n",
        "import diffrax\n",
        "from diffrax import ODETerm, Euler, Dopri5, AbstractSolver, diffeqsolve\n",
        "# from tensorflow_probability.substrates import jax as tfp\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Any, Union,Callable\n",
        "import chex\n",
        "from optax import ema\n",
        "from jax._src import prng\n",
        "\n",
        "from distrax import MultivariateNormalDiag\n",
        "\n",
        "from jax import lax, vmap\n",
        "jax.config.update(\"jax_enable_x64\", True)"
      ],
      "metadata": {
        "id": "fSlKGZO81RDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to accumulate the value fo all functionals\n",
        "@chex.dataclass\n",
        "class F_values:\n",
        "    energy: chex.ArrayDevice\n",
        "    kin: chex.ArrayDevice\n",
        "    vnuc: chex.ArrayDevice\n",
        "    hart: chex.ArrayDevice"
      ],
      "metadata": {
        "id": "ej6YMGN4K44X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following text is used to define the vector field needed in Normalizig flows.\n",
        "\n",
        "$$\n",
        "\\partial_t \\begin{bmatrix}\n",
        "\\mathbf{z}(t) \\\\\n",
        "\\log \\rho_\\phi(\\mathbf{z}(t))\n",
        "\\end{bmatrix} =\\begin{bmatrix}\n",
        "g_\\phi(\\mathbf{z}(t),t) \\\\\n",
        "-\\nabla_{\\mathbf{x}} \\cdot g_\\phi(\\mathbf{z}(t),t)\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "where $g_\\phi(\\mathbf{z}(t),t)$ is the NN that parametrizes the vector field, and the second term allows us to compute the change of volumne in Normalizing Flows."
      ],
      "metadata": {
        "id": "_oBsksitSiEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flow(nnx.Module):\n",
        "  def __init__(self, din: int, dim: int, rngs: nnx.Rngs):\n",
        "    self.din, self.dim = din, dim\n",
        "    self.linear_in = nnx.Linear(din + 1, dim, rngs=rngs)\n",
        "    self.blocks = [\n",
        "      nnx.Linear(dim, dim, rngs=rngs)\n",
        "      for _ in range(3)\n",
        "    ]\n",
        "    self.linear_out = nnx.Linear(dim, din, rngs=rngs)\n",
        "\n",
        "  def __call__(self, x, t):\n",
        "    x = jnp.concatenate([x, t], axis=-1)\n",
        "    x = self.linear_in(x)\n",
        "    x = jnp.tanh(x)\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "      x = jnp.tanh(x)\n",
        "    x = self.linear_out(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class CNF(nnx.Module):\n",
        "  def __init__(self, din: int, dim: int, rngs: nnx.Rngs):\n",
        "    self.din, self.dim = din, dim\n",
        "    self.flow = Flow(din, dim, rngs)\n",
        "\n",
        "  def __call__(self, states, t):\n",
        "\n",
        "    x, log_px = states[:-1], states[-1:]\n",
        "    dz, f_vjp = jax.vjp(self.flow, x,t)\n",
        "    x_ones = jnp.ones((self.din))\n",
        "    (dtrJ,_) = f_vjp(x_ones)\n",
        "    dtrJ = jnp.sum(dtrJ)\n",
        "\n",
        "    return jnp.concatenate([dz, -dtrJ[None]], axis=-1)\n",
        "\n",
        "data_dim: int = 1\n",
        "model_dim: int = 264\n",
        "rngs = nnx.Rngs(0)\n",
        "flow = Flow(data_dim, model_dim, rngs)\n",
        "flow_model = CNF(data_dim, model_dim, rngs)\n",
        "\n",
        "@nnx.vmap(in_axes=(None, 0, 0), out_axes=0)\n",
        "def forward(model, x, t):\n",
        "  return model(x,t)"
      ],
      "metadata": {
        "id": "bLXJ-tRnDKdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function generates random samples from the prior distribution which are used to compute the expectation value of the functionals."
      ],
      "metadata": {
        "id": "_AIz8mtLTs7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_generator(key: prng.PRNGKeyArray, batch_size: int, prior_dist: Callable):\n",
        "    \"\"\"\n",
        "    Generator that yields batches of samples from the prior distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key : prng.PRNGKeyArray\n",
        "        Key to generate random numbers.\n",
        "    batch_size : int\n",
        "        Size of the batch.\n",
        "    prior_dist : Callable\n",
        "        Prior distribution.\n",
        "\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        _, key = jrnd.split(key)\n",
        "        samples = prior_dist.sample(seed=key, sample_shape=batch_size)\n",
        "        logp_samples = prior_dist.log_prob(samples)\n",
        "        samples0 = lax.concatenate(\n",
        "            (samples, logp_samples[:,None]), 1)\n",
        "\n",
        "        _, key = jrnd.split(key)\n",
        "        samples = prior_dist.sample(seed=key, sample_shape=batch_size)\n",
        "        logp_samples = prior_dist.log_prob(samples)\n",
        "        samples1 = lax.concatenate(\n",
        "            (samples, logp_samples[:,None]), 1)\n",
        "\n",
        "        yield lax.concatenate((samples0, samples1), 0)\n",
        "\n",
        "key = jrnd.PRNGKey(0)\n",
        "_,key = jrnd.split(key)\n",
        "\n",
        "# information about LiH\n",
        "Ne = 2 # Number of valence electrons\n",
        "Z_alpha = 3 # Atomic number of Li\n",
        "Z_beta = 1 # Atomic number of H\n",
        "R =10. # Interatomic distance\n",
        "\n",
        "energies_ema = ema(decay=0.99)\n",
        "energies_state = energies_ema.init(\n",
        "    F_values(energy=jnp.array(0.), kin=jnp.array(0.), vnuc=jnp.array(0.),  hart = jnp.array(0.)))\n",
        "\n",
        "base_dist = distrax.MultivariateNormalDiag(jnp.array([0.]), jnp.array([1.]))"
      ],
      "metadata": {
        "id": "JRliLPII6cQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions are the functionals from our paper, Eqs. 11 to 21 in the Suplemental Information.\n",
        "[Paper link](https://arxiv.org/pdf/2404.08764)"
      ],
      "metadata": {
        "id": "aYbTSHc1UCsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def thomas_fermi_1D(den: Any, Ne: int, c: float=(jnp.pi*jnp.pi)/24) -> jax.Array:\n",
        "    r\"\"\"\n",
        "    Thomas-Fermi kinetic functional in 1D.\n",
        "    See original paper eq. 18 in https://pubs.aip.org/aip/jcp/article/139/22/224104/193579/Orbital-free-bond-breaking-via-machine-learning\n",
        "\n",
        "    T_{\\text{TF}}[\\rhom] = \\frac{\\pi^2}{24} \\int \\left(\\rhom(x) \\right)^{3} \\mathrm{d}x \\\\\n",
        "    T_{\\text{TF}}[\\rhom] = \\frac{\\pi^2}{24} \\Ne^3 \\EX_{\\rhozero} \\left[ (\\rhophi(x))^{2}\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    den : Array\n",
        "        Density.\n",
        "    score : Array\n",
        "        Gradient of the log-likelihood function.\n",
        "    Ne : int\n",
        "        Number of electrons.\n",
        "    c : float, optional\n",
        "        Multiplication constant, by default (jnp.pi*jnp.pi)/24\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    jax.Array\n",
        "        Thomas-Fermi kinetic energy.\n",
        "    \"\"\"\n",
        "\n",
        "    den_sqr = den*den\n",
        "    return c*(Ne**3)*den_sqr\n",
        "\n",
        "def soft_coulomb(x:Any,xp:Any,Ne: int) -> jax.Array:\n",
        "    r\"\"\"\n",
        "    Soft-Coulomb potential.\n",
        "\n",
        "    See eq 6 in https://pubs.aip.org/aip/jcp/article/139/22/224104/193579/Orbital-free-bond-breaking-via-machine-learning\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : Any\n",
        "        A point where the potential is evaluated.\n",
        "    xp : Any\n",
        "        A point where the charge density is zero.\n",
        "    Ne : int\n",
        "        Number of electrons.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    jax.Array\n",
        "        Soft version of the Coulomb potential.\n",
        "    \"\"\"\n",
        "    v_coul = 1/(jnp.sqrt( 1 + (x-xp)*(x-xp)))\n",
        "    return v_coul*Ne**2\n",
        "\n",
        "def attraction(x:Any, R:float, Z_alpha:int, Z_beta:int, Ne: int) -> jax.Array:\n",
        "    \"\"\"\n",
        "    Attraction between two nuclei.\n",
        "\n",
        "    See eq 7 in https://pubs.aip.org/aip/jcp/article/139/22/224104/193579/Orbital-free-bond-breaking-via-machine-learning\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : Any\n",
        "        A point where the potential is evaluated.\n",
        "    R : float\n",
        "        Distance between the two nuclei.\n",
        "    Z_alpha : int\n",
        "        Atomic number of the first nucleus.\n",
        "    Z_beta : int\n",
        "        Atomic number of the second nucleus.\n",
        "    Ne : int\n",
        "        Number of electrons.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    jax.Array\n",
        "        Attraction to the nuclei of charges Z_alpha and Z_beta.\n",
        "    \"\"\"\n",
        "    v_x = - Z_alpha/(jnp.sqrt(1 + (x + R/2)**2))  - Z_beta/(jnp.sqrt(1 + (x - R/2)**2))\n",
        "    return v_x*Ne"
      ],
      "metadata": {
        "id": "UfnPq20G6ILu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In continuous normalizing flows, we can move from the base distribution ($p_0(z)$) to the target ($p_x(x)$). \\\\\n",
        "For this we need to run the joint ODE in forward or reverse order.  "
      ],
      "metadata": {
        "id": "HEH9E7S7UlQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rev_ode(flow_model, z_and_logpz):\n",
        "  t0 = 0.\n",
        "  t1 = 1.\n",
        "  dt0 = t1 - t0\n",
        "  vector_field = lambda t, x, args: forward(flow_model, x, t*jnp.ones((x.shape[0],1)))\n",
        "  term = ODETerm(vector_field)\n",
        "  sol = diffeqsolve(term, diffrax.Tsit5(), t1, t0, -dt0, z_and_logpz, stepsize_controller=diffrax.PIDController(rtol=1e-6, atol=1e-6), saveat=diffrax.SaveAt(ts=jnp.array([1., 0.])))\n",
        "  x_and_logpx = sol.ys[-1,:,:]\n",
        "  x = x_and_logpx[:,:-1]\n",
        "  log_px = x_and_logpx[:,-1:]\n",
        "  return x,log_px\n",
        "\n",
        "def fwd_ode(flow_model, x_and_logpx):\n",
        "  t0 = 0.\n",
        "  t1 = 1.\n",
        "  dt0 = t1 - t0\n",
        "  flow_model.eval()\n",
        "  # vector_field = lambda t, x, args: flow_model(x, jnp.full(x.shape[0], t))\n",
        "  vector_field = lambda t, x, args: forward(flow_model, x, t*jnp.ones((x.shape[0],1)))\n",
        "  term = ODETerm(vector_field)\n",
        "  sol = diffeqsolve(term, diffrax.Tsit5(), t0, t1, dt0, x_and_logpx,\n",
        "                    stepsize_controller=diffrax.PIDController(rtol=1e-6, atol=1e-6),\n",
        "                    saveat=diffrax.SaveAt(ts=jnp.array([0., 1.])))\n",
        "  z_and_log_jac = sol.ys[-1,:,:]\n",
        "  z = z_and_log_jac[:,:-1]\n",
        "  log_jac = z_and_log_jac[:,-1:]\n",
        "  return z, log_jac"
      ],
      "metadata": {
        "id": "DQIP4iWMsGUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the density using numerical integration.\n",
        "def rho_rev(model, x):\n",
        "  zt = jnp.concatenate([x,jnp.zeros((x.shape[0],1))], axis=-1)\n",
        "  z0, logp_jac = rev_ode(model, zt)\n",
        "  logp_x = prior_dist.log_prob(z0)[:, None] - logp_jac\n",
        "  return jnp.exp(logp_x)\n",
        "\n",
        "def integral(model, x_and_logpx):\n",
        "  x = x_and_logpx[:,:-1]\n",
        "  p_x = rho_rev(model, x_and_logpx)\n",
        "  return jnp.trapezoid(p_x.flatten(),dx = x[1]-x[0]),p_x"
      ],
      "metadata": {
        "id": "pSNkkG8NsZm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# energy optimization\n",
        "def grad_loss(model, z_and_logpz):\n",
        "  x, log_px = fwd_ode(model, z_and_logpz)\n",
        "\n",
        "  den_all, x_all = jnp.exp(log_px), x\n",
        "  den, denp = den_all[:-1], den_all[-1:]\n",
        "  x, xp = x_all[:-1], x_all[-1:]\n",
        "\n",
        "  # evaluate all the functionals locally F[x_i, \\rho(x_i)]\n",
        "  e_t = thomas_fermi_1D(den, Ne)\n",
        "  e_h = soft_coulomb(x, xp, Ne)\n",
        "  e_nuc_v = attraction(x, R, Z_alpha, Z_beta,Ne)\n",
        "  e = e_t + e_nuc_v + e_h\n",
        "\n",
        "  energy = jnp.mean(e)\n",
        "\n",
        "  f_values = F_values(energy=energy,\n",
        "                            kin=jnp.mean(e_t),\n",
        "                            vnuc=jnp.mean(e_nuc_v),\n",
        "                            hart=jnp.mean(e_h),\n",
        "\n",
        "                            )\n",
        "\n",
        "  return energy, f_values\n",
        "\n",
        "@nnx.jit\n",
        "def train_step(flow_model: Flow, optimizer: nnx.Optimizer, x_and_logpx):\n",
        "  loss, grads = nnx.value_and_grad(grad_loss, has_aux=True)(flow_model, x_and_logpx)\n",
        "  optimizer.update(grads)\n",
        "  return loss, optimizer\n",
        "\n",
        "lr = optax.schedules.exponential_decay(2e-3, transition_steps = 1, decay_rate = 0.95)\n",
        "tx = optax.chain(\n",
        "    optax.clip_by_global_norm(1.0),\n",
        "    optax.adamw(lr,weight_decay=1E-5)\n",
        "        )\n",
        "optimizer = nnx.Optimizer(flow_model, tx)\n",
        "\n",
        "prior_dist = MultivariateNormalDiag(jnp.zeros(1), 1.*jnp.ones(1))\n",
        "\n",
        "gen_batches = batch_generator(key, model_dim, prior_dist)\n",
        "\n",
        "for itr in range(3):\n",
        "    _,key = jrnd.split(key)\n",
        "    batch = next(gen_batches) # generate a random sample form p_z\n",
        "\n",
        "    loss_value, optimizer = train_step(flow_model, optimizer, batch) # compute the energy\n",
        "    loss_epoch, losses = loss_value\n",
        "    energies_i_ema, energies_state = energies_ema.update(\n",
        "            losses, energies_state)\n",
        "    ei_ema = energies_i_ema.energy\n",
        "\n",
        "    r_ema = {'epoch': itr,\n",
        "                 'E': energies_i_ema.energy,\n",
        "                 'T': energies_i_ema.kin, 'V': energies_i_ema.vnuc, 'H': energies_i_ema.hart\n",
        "\n",
        "                 }\n",
        "    print( r_ema)\n"
      ],
      "metadata": {
        "id": "Wsu2MpE7lQ0I",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fc7d016-5d4d-4d96-93d2-969d745d6b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'E': Array(1.57444175, dtype=float64), 'T': Array(0.23048938, dtype=float64), 'V': Array(-1.6199868, dtype=float64), 'H': Array(2.96393917, dtype=float64)}\n",
            "{'epoch': 1, 'E': Array(-0.13409486, dtype=float64), 'T': Array(0.18302722, dtype=float64), 'V': Array(-2.65111808, dtype=float64), 'H': Array(2.333996, dtype=float64)}\n",
            "{'epoch': 2, 'E': Array(-0.52899626, dtype=float64), 'T': Array(0.19273305, dtype=float64), 'V': Array(-2.85198934, dtype=float64), 'H': Array(2.13026003, dtype=float64)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_grid = jnp.linspace(-12,12,1000)[:,None]\n",
        "norm_val, rho_pred = integral(flow_model,x_grid)\n",
        "print(norm_val)"
      ],
      "metadata": {
        "id": "xv57KEHMtUGg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "cd133037-f5e0-4dbe-c27f-3a54e95efec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "mul got incompatible shapes for broadcasting: (0,), (999,).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d7a426ff0d19>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnorm_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintegral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-21db0e21f3c1>\u001b[0m in \u001b[0;36mintegral\u001b[0;34m(model, x_and_logpx)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_and_logpx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mp_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho_rev\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_and_logpx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrapezoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mtrapezoid\u001b[0;34m(y, x, dx, axis)\u001b[0m\n\u001b[1;32m   7594\u001b[0m       \u001b[0mdx_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7595\u001b[0m   \u001b[0my_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmoveaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7596\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdx_array\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mop\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_forward_operator_to_aval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"_{name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_methods.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m     \u001b[0;31m# Note: don't use isinstance here, because we don't want to raise for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;31m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/ufunc_api.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"where argument of {self}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__static_props\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'call'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_vectorized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_argnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'self'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 13 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/ufuncs.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1254\u001b[0m   \"\"\"\n\u001b[1;32m   1255\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromote_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multiply\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbitwise_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36m_try_broadcast_shapes\u001b[0;34m(name, *shapes)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mresult_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_1s\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         raise TypeError(f'{name} got incompatible shapes for broadcasting: '\n\u001b[0m\u001b[1;32m    129\u001b[0m                         f'{\", \".join(map(str, map(tuple, shapes)))}.')\n\u001b[1;32m    130\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (0,), (999,)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x_grid,rho_pred.flatten(),label='Normalizing Flow')\n",
        "p0 = prior_dist.prob(x_grid)\n",
        "plt.plot(x_grid,p0,label='Base distribution')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "bAn-uQWNtqTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mAhSxHYOWBDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}